# GenAI Session Analyzer â€” CLAUDE.md

## Persona

You are a **Staff Data Architect** helping me build a production-grade portfolio project in 48 hours. I'm interviewing for a **Lead Data/Strategy role at Luma AI** (high-growth GenAI startup). Due to NDAs, I cannot show actual workplace codeâ€”this "Clean Room" project demonstrates my engineering instincts.

**Your communication style:**
- Direct, opinionated, and pragmatic
- Favor speed-to-demo over perfection
- Call out over-engineering immediately
- Suggest the "80/20 solution" first, then mention the "production version" as context
- When I'm stuck, give me the codeâ€”don't just describe it

---

## Project Mission

Build the **GenAI Session Analyzer**: a local data platform that ingests real prompt data (DiffusionDB from HuggingFace), enriches it with synthetic telemetry (latency, errors, user cohorts), and exposes a dashboard showing **User Friction** and **Session Costs**.

**Interview Alignment (what Luma evaluators care about):**
1. Clear understanding of requirements and stakeholder needs
2. Strong data modeling and transformation instincts
3. Thoughtful pipeline design and orchestration choices
4. Balance of engineering rigor with analytical usability
5. Clear communication of complex systems through code

---

## Tech Stack (Speed-Optimized for 48hrs)

| Layer | Tool | Why |
|-------|------|-----|
| **Ingestion** | `datasets` (HuggingFace) | One-liner to stream DiffusionDB |
| **Storage** | DuckDB | Local OLAP, zero config, SQL-native, parquet-friendly |
| **Image Storage** | Local filesystem | `/data/images/` with FK references in DuckDB |
| **Orchestration** | Dagster | Modern, asset-based, beautiful local UI, interview-impressive |
| **Transformation** | dbt-duckdb | Industry-standard modeling, star schema, data tests |
| **ML Enrichment** | MLX + sentence-transformers | LLM prompt analysis + semantic search embeddings |
| **Simulation** | Faker + NumPy | Fast synthetic data generation with realistic distributions |
| **Dashboard** | Streamlit | Fast to build, can display images alongside metrics |
| **Environment** | uv + pyproject.toml | Modern Python packaging, fast dependency resolution |

**Not using (and why):**
- Airflow: Overkill for local, slower to set up
- Spark: Unnecessary scale for this data volume
- Postgres: DuckDB is faster for analytics and zero-config
- HuggingFace Transformers: MLX is 3-4x faster on Apple Silicon

---

## Data Architecture

### Source Data
```
DiffusionDB (HuggingFace) â€” 10K samples
â”œâ”€â”€ prompt (text)
â”œâ”€â”€ image (PIL Image)
â”œâ”€â”€ seed (int)
â”œâ”€â”€ cfg (float)
â”œâ”€â”€ sampler (string)
â””â”€â”€ width/height (int)
```

### Storage Layout
```
data/
â”œâ”€â”€ warehouse.duckdb              # All metadata, embeddings, analysis
â””â”€â”€ blob/                         # Simulated blob storage (like S3)
    â””â”€â”€ images/
        â””â”€â”€ generations/          # Images from DiffusionDB
            â”œâ”€â”€ gen_00001.png
            â”œâ”€â”€ gen_00002.png
            â””â”€â”€ ... (10K images, ~5GB)
```

**Production note:** Local `blob/` directory simulates S3. In production, swap `Path(image_path)` for `s3.get_object()`. Same pattern, different backend.

### Dimensional Model (Star Schema)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   dim_users     â”‚      â”‚          dim_prompts                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ user_id (PK)    â”‚      â”‚ prompt_id (PK)                      â”‚
â”‚ user_tier       â”‚      â”‚ prompt_text                         â”‚
â”‚ signup_date     â”‚      â”‚ image_path                          â”‚
â”‚ cohort_week     â”‚      â”‚                                     â”‚
â”‚ region          â”‚      â”‚ -- LLM-extracted (pre-computed) --  â”‚
â”‚ device_type     â”‚      â”‚ subject                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚ art_style                           â”‚
         â”‚               â”‚ mood                                â”‚
         â”‚               â”‚ complexity_score (1-5)              â”‚
         â”‚               â”‚ has_nsfw_intent (bool)              â”‚
         â”‚               â”‚ setting                             â”‚
         â”‚               â”‚ lighting                            â”‚
         â”‚               â”‚                                     â”‚
         â”‚               â”‚ -- CLIP (pre-computed) --           â”‚
         â”‚               â”‚ image_embedding (FLOAT[512])        â”‚
         â”‚               â”‚ text_embedding (FLOAT[512])         â”‚
         â”‚               â”‚ alignment_score (prompt vs image)   â”‚
         â”‚               â”‚ style_cluster (k-means cluster ID)  â”‚
         â”‚               â”‚                                     â”‚
         â”‚               â”‚ -- Semantic Search (pre-computed) --â”‚
         â”‚               â”‚ text_embedding_mini (FLOAT[384])    â”‚
         â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                â”‚
         â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚    â”‚
         â–¼    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              fct_generations                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ generation_id (PK)                          â”‚
â”‚ user_id (FK)                                â”‚
â”‚ prompt_id (FK)                              â”‚
â”‚ session_id                                  â”‚
â”‚ timestamp                                   â”‚
â”‚ latency_ms                                  â”‚
â”‚ status (success/timeout/safety_violation/   â”‚
â”‚         rate_limited/model_error)           â”‚
â”‚ cost_credits                                â”‚
â”‚ model_version                               â”‚
â”‚ retry_count                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              fct_sessions (aggregated)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ session_id (PK)                             â”‚
â”‚ user_id (FK)                                â”‚
â”‚ session_start                               â”‚
â”‚ session_end                                 â”‚
â”‚ total_generations                           â”‚
â”‚ success_rate                                â”‚
â”‚ avg_latency_ms                              â”‚
â”‚ total_cost_credits                          â”‚
â”‚ friction_score (derived)                    â”‚
â”‚ churned_after (boolean)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Business Metrics
- **Friction Score**: Weighted composite of (error_rate Ã— 3) + (avg_latency_normalized Ã— 2) + (retry_rate Ã— 1)
- **Session Cost**: Sum of cost_credits per session (based on prompt tokens + generation time)
- **Churn Indicator**: No activity for 7+ days after session
- **Alignment Score**: CLIP cosine similarity between prompt text and generated image (0-1)
- **Style Clusters**: K-means groupings of image embeddings for visual categorization
- **Semantic Search**: Find similar prompts using sentence-transformer embeddings (RAG pattern)

---

## Simulation Logic Strategy

### 1. User Generation
```python
# Cohort distribution (realistic power law)
user_tiers = {
    'free': 0.70,      # Casual users
    'pro': 0.25,       # Regular users
    'enterprise': 0.05 # Power users
}
```

### 2. Status Determination (from prompt text)
```python
def determine_status(prompt_text: str, user_tier: str) -> str:
    """
    Heuristics based on prompt characteristics:
    """
    prompt_lower = prompt_text.lower()
    token_count = len(prompt_text.split())

    # Safety violation signals (NSFW keywords, violent content)
    nsfw_keywords = ['nude', 'naked', 'nsfw', 'explicit', ...]
    if any(kw in prompt_lower for kw in nsfw_keywords):
        return 'safety_violation' if random.random() < 0.85 else 'success'

    # Timeout signals (very long/complex prompts)
    if token_count > 75:
        return 'timeout' if random.random() < 0.15 else 'success'

    # Rate limiting (free tier + high frequency)
    if user_tier == 'free':
        return 'rate_limited' if random.random() < 0.08 else 'success'

    # Random model errors (baseline noise)
    if random.random() < 0.02:
        return 'model_error'

    return 'success'
```

### 3. Latency Distribution
```python
def generate_latency(prompt_text: str, status: str) -> int:
    """Realistic latency based on prompt complexity and outcome"""
    base_latency = len(prompt_text.split()) * 50  # ~50ms per token

    if status == 'timeout':
        return 30000  # 30s timeout
    elif status == 'safety_violation':
        return random.randint(100, 500)  # Fast rejection
    else:
        # Log-normal distribution (realistic API latency)
        noise = np.random.lognormal(0, 0.5)
        return int(base_latency * noise)
```

### 4. Session Logic
```python
# Session = prompts from same user within 30-min window
SESSION_TIMEOUT_MINUTES = 30

# Churn probability based on friction
def calculate_churn_probability(friction_score: float, user_tier: str) -> float:
    base_churn = {'free': 0.40, 'pro': 0.15, 'enterprise': 0.05}
    return min(0.95, base_churn[user_tier] * (1 + friction_score))
```

---

## ML Enrichment Layer (Pre-computed with MLX)

**Strategy:** Run ML models once on Saturday night, store results as columns. Zero model loading during demoâ€”just fast SQL queries.

### Setup (M5 MacBook Pro)
```bash
# Install MLX ecosystem
pip install mlx mlx-lm mlx-clip

# Models will auto-download on first run (~3GB total)
```

### Time Budget for 10K Items

| Task | Per Item | 10K Total |
|------|----------|-----------|
| Download images from HuggingFace | ~50ms | ~10 min |
| Save images to disk | ~20ms | ~3 min |
| CLIP embeddings (MLX) | ~30ms | ~5 min |
| LLM prompt analysis (MLX) | ~300ms | ~50 min |
| **Total** | | **~70 min** |

### 1. CLIP Embeddings (Image + Text)

**Model:** `mlx-community/clip-vit-base-patch32` (~400MB)

**What it produces:**
- `image_embedding`: 512-dim vector for the generated image
- `text_embedding`: 512-dim vector for the prompt text
- `alignment_score`: Cosine similarity (did model follow the prompt?)

```python
# precompute_clip.py
from mlx_clip import load, image_encoder, text_encoder
from PIL import Image
import numpy as np

model = load("mlx-community/clip-vit-base-patch32")

def compute_clip_features(image_path: str, prompt_text: str) -> dict:
    image = Image.open(image_path)

    image_emb = image_encoder(model, image)
    text_emb = text_encoder(model, prompt_text)

    # Normalize and compute alignment
    image_emb = image_emb / np.linalg.norm(image_emb)
    text_emb = text_emb / np.linalg.norm(text_emb)
    alignment = np.dot(image_emb, text_emb)

    return {
        'image_embedding': image_emb.tolist(),
        'text_embedding': text_emb.tolist(),
        'alignment_score': float(alignment)
    }
```

**What you can query:**
```sql
-- Images with poor prompt adherence
SELECT prompt_text, image_path, alignment_score
FROM dim_prompts
WHERE alignment_score < 0.2
ORDER BY alignment_score;

-- Find similar images (cosine similarity)
SELECT b.prompt_id, b.image_path,
       list_cosine_similarity(a.image_embedding, b.image_embedding) AS similarity
FROM dim_prompts a, dim_prompts b
WHERE a.prompt_id = 42 AND b.prompt_id != 42
ORDER BY similarity DESC
LIMIT 5;
```

### 2. LLM Prompt Analysis

**Model:** `mlx-community/Qwen2.5-1.5B-Instruct-4bit` (~1GB)

**What it extracts:**
- `subject`: Main subject (person, landscape, object, etc.)
- `art_style`: Visual style (photorealistic, anime, oil painting, etc.)
- `mood`: Emotional tone (dark, cheerful, mysterious, etc.)
- `complexity_score`: 1-5 based on prompt detail level
- `has_nsfw_intent`: Boolean safety flag
- `setting`: Location/environment
- `lighting`: Lighting description if present

```python
# precompute_llm.py
from mlx_lm import load, generate
import json

model, tokenizer = load("mlx-community/Qwen2.5-1.5B-Instruct-4bit")

EXTRACTION_PROMPT = """Analyze this image generation prompt. Return ONLY valid JSON.

Prompt: "{prompt}"

{{"subject": "main subject", "style": "art style", "mood": "tone", "complexity": 1-5, "has_nsfw_intent": true/false, "setting": "location", "lighting": "lighting type"}}"""

def analyze_prompt(prompt_text: str) -> dict:
    prompt = EXTRACTION_PROMPT.format(prompt=prompt_text)
    response = generate(model, tokenizer, prompt=prompt, max_tokens=100)

    try:
        json_str = response[response.find('{'):response.rfind('}')+1]
        return json.loads(json_str)
    except:
        return {
            "subject": "unknown", "style": "unknown", "mood": "unknown",
            "complexity": 3, "has_nsfw_intent": False, "setting": "unknown", "lighting": "unknown"
        }
```

**What you can query:**
```sql
-- Do complex prompts timeout more?
SELECT complexity_score,
       AVG(latency_ms) AS avg_latency,
       SUM(CASE WHEN status = 'timeout' THEN 1 ELSE 0 END)::FLOAT / COUNT(*) AS timeout_rate
FROM fct_generations g
JOIN dim_prompts p USING (prompt_id)
GROUP BY complexity_score;

-- What styles do power users prefer?
SELECT u.user_tier, p.art_style, COUNT(*) AS count
FROM fct_generations g
JOIN dim_users u USING (user_id)
JOIN dim_prompts p USING (prompt_id)
GROUP BY u.user_tier, p.art_style
ORDER BY u.user_tier, count DESC;

-- NSFW attempt rate by user tier
SELECT user_tier,
       SUM(CASE WHEN has_nsfw_intent THEN 1 ELSE 0 END) AS nsfw_attempts,
       ROUND(100.0 * SUM(CASE WHEN has_nsfw_intent THEN 1 ELSE 0 END) / COUNT(*), 2) AS nsfw_rate_pct
FROM fct_generations g
JOIN dim_users u USING (user_id)
JOIN dim_prompts p USING (prompt_id)
GROUP BY user_tier;
```

### 3. Style Clustering (Post-CLIP)

After CLIP embeddings are computed, run k-means to group images by visual style:

```python
from sklearn.cluster import KMeans
import numpy as np

# Load embeddings from DuckDB
embeddings = duckdb.sql("SELECT prompt_id, image_embedding FROM dim_prompts").df()
X = np.array(embeddings['image_embedding'].tolist())

# Cluster into 8 style groups
kmeans = KMeans(n_clusters=8, random_state=42)
clusters = kmeans.fit_predict(X)

# Update DuckDB
for prompt_id, cluster in zip(embeddings['prompt_id'], clusters):
    duckdb.sql(f"UPDATE dim_prompts SET style_cluster = {cluster} WHERE prompt_id = {prompt_id}")
```

**What you can query:**
```sql
-- Which style clusters have highest friction?
SELECT p.style_cluster,
       COUNT(*) AS generations,
       AVG(s.friction_score) AS avg_friction
FROM fct_generations g
JOIN dim_prompts p USING (prompt_id)
JOIN fct_sessions s USING (session_id)
GROUP BY p.style_cluster
ORDER BY avg_friction DESC;
```

### Production Alternative (Interview Talking Point)

> "For local development, I used MLXâ€”it's optimized for Apple Silicon and let me pre-compute 10K enrichments in about an hour.
>
> In production, I'd swap to **vLLM** or **HuggingFace TGI** on GPU clusters for self-hosted, or use **Claude API / Amazon Bedrock** for managed inference. The enrichment logic stays the sameâ€”only the inference backend changes."

---

## SQL Copilot (Live Demo Feature)

**What it does:** User types a natural language question â†’ Local LLM generates SQL â†’ Human reviews â†’ Execute against DuckDB.

**Why it's impressive:** Shows real-time LLM integration, not just pre-computed features.

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Streamlit UI                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ "Show me error rate for power users"                â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                            â”‚                                â”‚
â”‚                            â–¼                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ construct_prompt(question, schema)                  â”‚   â”‚
â”‚  â”‚ Injects table schema so LLM knows column names      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                            â”‚                                â”‚
â”‚                            â–¼                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ MLX + Qwen2.5-1.5B-Instruct                         â”‚   â”‚
â”‚  â”‚ Generates SQL string (~0.5s on M5)                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                            â”‚                                â”‚
â”‚                            â–¼                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ st.text_area (editable)                             â”‚   â”‚
â”‚  â”‚ SELECT user_tier, AVG(error_rate)...                â”‚   â”‚
â”‚  â”‚                                    [Run Query]      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                            â”‚                                â”‚
â”‚                            â–¼                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ DuckDB executes â†’ Results displayed                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1. Schema Extractor

```python
# src/copilot/schema.py

import duckdb

def get_duckdb_schema(con: duckdb.DuckDBPyConnection, table_name: str = "fct_generations") -> str:
    """
    Extract table schema and format for LLM consumption.
    """
    schema_df = con.execute(f"DESCRIBE {table_name}").fetchdf()

    # Format as CREATE TABLE (LLMs understand this well)
    columns = []
    for _, row in schema_df.iterrows():
        col_name = row['column_name']
        col_type = row['column_type']
        columns.append(f"    {col_name} {col_type}")

    schema_text = f"CREATE TABLE {table_name} (\n"
    schema_text += ",\n".join(columns)
    schema_text += "\n);"

    return schema_text


def get_full_schema(con: duckdb.DuckDBPyConnection) -> str:
    """Get schema for all tables."""
    tables = ['dim_users', 'dim_prompts', 'fct_generations', 'fct_sessions']
    schemas = [get_duckdb_schema(con, t) for t in tables]
    return "\n\n".join(schemas)
```

### 2. Prompt Template

```python
# src/copilot/prompt.py

def construct_prompt(user_question: str, schema_text: str) -> str:
    """
    Construct prompt optimized for SQL generation.
    Format works well with SQLCoder, Llama-3, Qwen2.5.
    """
    prompt = f"""### Task
Generate a SQL query to answer the following question:
{user_question}

### Database Schema
The query will run on a DuckDB database with the following schema:
{schema_text}

### Instructions
- Use DuckDB SQL syntax (LIMIT not TOP, ILIKE for case-insensitive)
- Return only the SQL query, no explanations
- Do not wrap in markdown code blocks

### Answer
SELECT"""

    return prompt
```

### 3. LLM Integration (MLX)

```python
# src/copilot/llm.py

from mlx_lm import load, generate

# Load model once at app startup (cache in st.session_state)
@st.cache_resource
def load_copilot_model():
    model, tokenizer = load("mlx-community/Qwen2.5-1.5B-Instruct-4bit")
    return model, tokenizer


def get_llm_response(prompt: str, model, tokenizer) -> str:
    """Generate SQL from prompt using MLX."""
    response = generate(
        model,
        tokenizer,
        prompt=prompt,
        max_tokens=200,
        temp=0.1  # Low temp for deterministic SQL
    )
    return clean_response(response)


def clean_response(response: str) -> str:
    """Clean LLM output to extract SQL."""
    sql = response.strip()

    # Remove markdown if present
    if sql.startswith('```'):
        sql = sql.split('```')[1]
        if sql.startswith('sql'):
            sql = sql[3:]

    # Prompt ends with "SELECT", so prepend if missing
    if not sql.upper().startswith('SELECT'):
        sql = 'SELECT ' + sql

    # Ensure ends with semicolon
    if not sql.strip().endswith(';'):
        sql = sql.strip() + ';'

    return sql
```

### 4. Streamlit Integration

```python
# dashboard/app.py (Copilot Section)

import streamlit as st
import duckdb

# --- SQL Copilot Section ---
st.header("ğŸ¤– SQL Copilot")
st.caption("Ask questions in natural language, get SQL queries")

# Load model (cached)
model, tokenizer = load_copilot_model()

# Get schema (cached)
@st.cache_data
def get_schema():
    con = duckdb.connect("data/warehouse.duckdb", read_only=True)
    return get_full_schema(con)

schema_text = get_schema()

# User input
user_question = st.text_input(
    "Ask a question about your data:",
    placeholder="e.g., Show me error rate by user tier"
)

if user_question:
    # Generate SQL
    with st.spinner("Generating SQL..."):
        prompt = construct_prompt(user_question, schema_text)
        generated_sql = get_llm_response(prompt, model, tokenizer)

    # Editable SQL area
    st.subheader("Generated SQL")
    edited_sql = st.text_area(
        "Review and edit if needed:",
        value=generated_sql,
        height=150
    )

    # Run button
    col1, col2 = st.columns([1, 5])
    with col1:
        run_button = st.button("â–¶ï¸ Run Query", type="primary")

    if run_button:
        try:
            # Validate (block DROP, DELETE, etc.)
            sql_upper = edited_sql.upper()
            if any(kw in sql_upper for kw in ['DROP', 'DELETE', 'UPDATE', 'INSERT']):
                st.error("âš ï¸ Dangerous operation blocked. SELECT queries only.")
            else:
                # Execute
                con = duckdb.connect("data/warehouse.duckdb", read_only=True)
                result_df = con.execute(edited_sql).fetchdf()

                st.subheader("Results")
                st.dataframe(result_df, use_container_width=True)

                # Show row count
                st.caption(f"Returned {len(result_df)} rows")

        except Exception as e:
            st.error(f"Query error: {str(e)}")
```

### Example Queries to Demo

| Natural Language | Expected SQL |
|------------------|--------------|
| "Show me error rate by user tier" | `SELECT user_tier, AVG(CASE WHEN status != 'success' THEN 1 ELSE 0 END) AS error_rate FROM fct_generations JOIN dim_users USING (user_id) GROUP BY user_tier` |
| "Top 10 users by session cost" | `SELECT user_id, SUM(total_cost_credits) AS total_cost FROM fct_sessions GROUP BY user_id ORDER BY total_cost DESC LIMIT 10` |
| "What art styles have highest latency?" | `SELECT art_style, AVG(latency_ms) AS avg_latency FROM fct_generations JOIN dim_prompts USING (prompt_id) GROUP BY art_style ORDER BY avg_latency DESC` |
| "Count of generations per day" | `SELECT DATE_TRUNC('day', timestamp) AS day, COUNT(*) FROM fct_generations GROUP BY day ORDER BY day` |

### Performance on M5

| Metric | Value |
|--------|-------|
| Model load (first time) | ~3s |
| Model load (cached) | ~0.1s |
| SQL generation | ~0.3-0.5s |
| Total response time | **< 1s** |

### Safety Rails

```python
BLOCKED_KEYWORDS = ['DROP', 'DELETE', 'UPDATE', 'INSERT', 'ALTER', 'TRUNCATE']

def validate_sql(sql: str) -> tuple[bool, str]:
    """Block dangerous operations."""
    sql_upper = sql.upper()
    for kw in BLOCKED_KEYWORDS:
        if kw in sql_upper:
            return False, f"Blocked: {kw} not allowed"
    if not sql_upper.strip().startswith('SELECT'):
        return False, "Only SELECT queries allowed"
    return True, ""
```

### Interview Talking Point

> "The SQL Copilot runs a local LLM in real-timeâ€”no API calls, no latency. I inject the database schema into the prompt so the model knows the column names. The human reviews the SQL before execution, and I block any dangerous operations like DROP or DELETE. Generation takes about 300ms on my M5 Mac."

---

## Session Explorer (AI-Powered Search + Filters + Images)

**What it does:** Semantic search + LLM-generated filters + image preview. A full data exploration tool for GenAI sessions.

**The evolution:**
- ~~Basic semantic search~~ â†’ **Full AI-powered explorer**
- Vector similarity + filter by style/mood/status + see actual images

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Session Explorer UI                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Search: "cyberpunk city"                                 â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Style:      â”‚ â”‚ Mood:       â”‚ â”‚ Status:     â”‚ â”‚ Tier:     â”‚ â”‚
â”‚  â”‚ [All     â–¼] â”‚ â”‚ [All     â–¼] â”‚ â”‚ [All     â–¼] â”‚ â”‚ [All   â–¼] â”‚ â”‚
â”‚  â”‚ anime       â”‚ â”‚ dark        â”‚ â”‚ success     â”‚ â”‚ free      â”‚ â”‚
â”‚  â”‚ realistic   â”‚ â”‚ cheerful    â”‚ â”‚ timeout     â”‚ â”‚ pro       â”‚ â”‚
â”‚  â”‚ painting    â”‚ â”‚ mysterious  â”‚ â”‚ safety      â”‚ â”‚ enterpriseâ”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                        (LLM-generated labels)                   â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Results: 847 similar prompts (showing top 12)            â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚  â”‚
â”‚  â”‚ â”‚  IMG    â”‚ â”‚  IMG    â”‚ â”‚  IMG    â”‚ â”‚  IMG    â”‚         â”‚  â”‚
â”‚  â”‚ â”‚         â”‚ â”‚         â”‚ â”‚         â”‚ â”‚         â”‚         â”‚  â”‚
â”‚  â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”‚  â”‚
â”‚  â”‚ â”‚neon     â”‚ â”‚blade    â”‚ â”‚future   â”‚ â”‚cyber    â”‚         â”‚  â”‚
â”‚  â”‚ â”‚tokyo... â”‚ â”‚runner...â”‚ â”‚city...  â”‚ â”‚woman... â”‚         â”‚  â”‚
â”‚  â”‚ â”‚Sim: 0.85â”‚ â”‚Sim: 0.82â”‚ â”‚Sim: 0.81â”‚ â”‚Sim: 0.79â”‚         â”‚  â”‚
â”‚  â”‚ â”‚âœ… 2.3s  â”‚ â”‚âœ… 1.9s  â”‚ â”‚â±ï¸ 31s   â”‚ â”‚ğŸ›‘ safetyâ”‚         â”‚  â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ ğŸ“Š Cluster Insights                                      â”‚  â”‚
â”‚  â”‚ Error Rate: 23%  |  Avg Latency: 4.2s  |  Top Style: animeâ”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Storage Layout (Simulated Blob Storage)

```
data/
â”œâ”€â”€ warehouse.duckdb              # Metadata + embeddings
â””â”€â”€ blob/                         # Simulated blob storage (like S3)
    â””â”€â”€ images/
        â””â”€â”€ generations/          # Generated images from DiffusionDB
            â”œâ”€â”€ gen_00001.png
            â”œâ”€â”€ gen_00002.png
            â””â”€â”€ ... (10K images)
```

**Interview note:** "The images live in a local blob directory that simulates S3. In production, you'd swap `Path(image_path)` for `s3.get_object()`. Same pattern, different backend."

### Implementation

```python
# dashboard/app.py â€” Session Explorer Section

import streamlit as st
import duckdb
from sentence_transformers import SentenceTransformer
from pathlib import Path

st.header("ğŸ” Session Explorer")
st.caption("Semantic search + LLM-generated filters + image preview")

# --- Setup ---
@st.cache_resource
def load_embedding_model():
    return SentenceTransformer('all-MiniLM-L6-v2')

@st.cache_resource
def get_connection():
    con = duckdb.connect("data/warehouse.duckdb", read_only=True)
    con.execute("LOAD vss;")
    return con

model = load_embedding_model()
con = get_connection()

# --- Get filter options from LLM-generated labels ---
@st.cache_data
def get_filter_options():
    styles = con.execute("SELECT DISTINCT art_style FROM dim_prompts WHERE art_style IS NOT NULL").fetchall()
    moods = con.execute("SELECT DISTINCT mood FROM dim_prompts WHERE mood IS NOT NULL").fetchall()
    statuses = con.execute("SELECT DISTINCT status FROM fct_generations").fetchall()
    tiers = con.execute("SELECT DISTINCT user_tier FROM dim_users").fetchall()
    return {
        'styles': ['All'] + [r[0] for r in styles],
        'moods': ['All'] + [r[0] for r in moods],
        'statuses': ['All'] + [r[0] for r in statuses],
        'tiers': ['All'] + [r[0] for r in tiers]
    }

filters = get_filter_options()

# --- Search Input ---
search_query = st.text_input(
    "Search prompts:",
    placeholder="e.g., cyberpunk city, fantasy castle, portrait"
)

# --- Filter Row (powered by LLM-extracted labels) ---
col1, col2, col3, col4 = st.columns(4)
with col1:
    style_filter = st.selectbox("Art Style", filters['styles'])
with col2:
    mood_filter = st.selectbox("Mood", filters['moods'])
with col3:
    status_filter = st.selectbox("Status", filters['statuses'])
with col4:
    tier_filter = st.selectbox("User Tier", filters['tiers'])

# --- Search Logic ---
if search_query:
    # Embed query on-the-fly (~30ms)
    query_embedding = model.encode([search_query])[0].tolist()

    # Build dynamic WHERE clause from filters
    where_clauses = []
    if style_filter != 'All':
        where_clauses.append(f"p.art_style = '{style_filter}'")
    if mood_filter != 'All':
        where_clauses.append(f"p.mood = '{mood_filter}'")
    if status_filter != 'All':
        where_clauses.append(f"g.status = '{status_filter}'")
    if tier_filter != 'All':
        where_clauses.append(f"u.user_tier = '{tier_filter}'")

    where_sql = " AND ".join(where_clauses) if where_clauses else "1=1"

    # DuckDB VSS search with filters (~2ms)
    results = con.execute(f"""
        SELECT
            p.prompt_id,
            p.prompt_text,
            p.image_path,
            p.art_style,
            p.mood,
            p.complexity_score,
            u.user_tier,
            g.latency_ms,
            g.status,
            array_cosine_similarity(p.text_embedding_mini, $1::FLOAT[384]) AS similarity
        FROM dim_prompts p
        JOIN fct_generations g USING (prompt_id)
        JOIN dim_users u USING (user_id)
        WHERE {where_sql}
        ORDER BY similarity DESC
        LIMIT 12
    """, [query_embedding]).fetchdf()

    # --- Cluster Insights ---
    st.subheader(f"Results for '{search_query}'")

    if len(results) == 0:
        st.warning("No results found. Try adjusting filters.")
    else:
        # Metrics row
        col1, col2, col3 = st.columns(3)
        error_rate = (results['status'] != 'success').mean()
        avg_latency = results['latency_ms'].mean()
        top_style = results['art_style'].mode().values[0] if len(results) > 0 else "N/A"

        col1.metric("Error Rate", f"{error_rate:.1%}")
        col2.metric("Avg Latency", f"{avg_latency:,.0f}ms")
        col3.metric("Top Style", top_style)

        st.divider()

        # --- Image Grid ---
        cols = st.columns(4)
        for idx, row in results.iterrows():
            with cols[idx % 4]:
                # Load image from local blob storage
                image_path = Path(row['image_path'])
                if image_path.exists():
                    st.image(str(image_path), use_container_width=True)
                else:
                    st.image("https://via.placeholder.com/256", use_container_width=True)

                # Status indicator
                status_emoji = {
                    'success': 'âœ…',
                    'timeout': 'â±ï¸',
                    'safety_violation': 'ğŸ›‘',
                    'rate_limited': 'âš ï¸',
                    'model_error': 'âŒ'
                }.get(row['status'], 'â“')

                st.caption(f"**{row['prompt_text'][:35]}...**")
                st.caption(f"Sim: {row['similarity']:.2f} | {status_emoji} {row['latency_ms']:,}ms")
                st.caption(f"{row['art_style']} â€¢ {row['mood']}")
```

### Why This Is Powerful

| Feature | What It Shows | Business Value |
|---------|---------------|----------------|
| Semantic search | Vector embeddings + DuckDB VSS | "Find prompts like X" |
| LLM-generated filters | Pre-computed labels from Qwen | "Filter by style, mood, complexity" |
| Status filter | Telemetry integration | "Show me what's failing" |
| Image preview | Blob storage pattern | "Actually see the outputs" |
| Cluster metrics | Real-time aggregation | "This cluster has 40% error rate" |

### Example Demo Flow

```
You: "Let me show you the Session Explorer..."

[Types "cyberpunk city"]
[Selects: Status = "timeout"]

Results appear instantly with images:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ–¼ï¸      â”‚ â”‚ ğŸ–¼ï¸      â”‚ â”‚ ğŸ–¼ï¸      â”‚ â”‚ ğŸ–¼ï¸      â”‚
â”‚neon     â”‚ â”‚future   â”‚ â”‚blade    â”‚ â”‚dystopia â”‚
â”‚tokyo... â”‚ â”‚city...  â”‚ â”‚runner...â”‚ â”‚mega...  â”‚
â”‚â±ï¸ 31s   â”‚ â”‚â±ï¸ 30s   â”‚ â”‚â±ï¸ 32s   â”‚ â”‚â±ï¸ 30s   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Error Rate: 100%  |  Avg Latency: 30,750ms  |  Top Style: realistic

You: "All the timeouts for 'cyberpunk city' are realistic style prompts
      with high complexity. The model might be struggling with detailed
      cyberpunk scenes. Let me check if anime style does better..."

[Changes filter: Style = "anime"]

Error Rate: 12%  |  Avg Latency: 2,100ms

You: "Anime cyberpunk prompts have only 12% error rate.
      This tells us the issue is style-specific, not prompt-specific."
```

### Performance

| Step | Time |
|------|------|
| Embed search query | ~30ms |
| DuckDB VSS + filters | ~2ms |
| Load 12 images | ~100ms |
| **Total response** | **< 150ms** |

### Interview Talking Point

> "Session Explorer combines three AI techniques: vector embeddings for semantic search, LLM-extracted labels for filtering, and a blob storage pattern for images.
>
> The filtersâ€”art style, mood, complexityâ€”were all extracted by a local LLM during preprocessing. At query time, I combine vector similarity with SQL filters in a single DuckDB query.
>
> This lets me answer questions like 'show me failed cyberpunk generations in anime style'â€”and actually see the images. That's how you debug a GenAI system."

---

## Project Structure

```
genai-session-analyzer/
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â”œâ”€â”€ CLAUDE.md                    # This file
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ diffusiondb_loader.py
â”‚   â”‚
â”‚   â”œâ”€â”€ enrichment/              # ML feature extraction (pre-compute)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ precompute_clip.py   # CLIP embeddings + alignment scores
â”‚   â”‚   â”œâ”€â”€ precompute_llm.py    # LLM prompt analysis
â”‚   â”‚   â”œâ”€â”€ precompute_text_embeddings.py  # sentence-transformers for search
â”‚   â”‚   â””â”€â”€ precompute_clusters.py # K-means style clustering
â”‚   â”‚
â”‚   â”œâ”€â”€ copilot/                 # SQL Copilot (live inference)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ schema.py            # get_duckdb_schema()
â”‚   â”‚   â”œâ”€â”€ prompt.py            # construct_prompt()
â”‚   â”‚   â””â”€â”€ llm.py               # get_llm_response() via MLX
â”‚   â”‚
â”‚   â”œâ”€â”€ simulation/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ user_generator.py
â”‚   â”‚   â”œâ”€â”€ telemetry_enricher.py
â”‚   â”‚   â””â”€â”€ session_builder.py
â”‚   â”‚
â”‚   â””â”€â”€ pipeline/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ definitions.py       # Dagster assets
â”‚       â””â”€â”€ resources.py
â”‚
â”œâ”€â”€ dashboard/
â”‚   â””â”€â”€ app.py                   # Streamlit: metrics + SQL Copilot + Session Explorer
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ warehouse.duckdb         # All metadata, embeddings, analysis
â”‚   â””â”€â”€ blob/                    # Simulated blob storage (like S3)
â”‚       â””â”€â”€ images/
â”‚           â””â”€â”€ generations/     # 10K images from DiffusionDB (~5GB)
â”‚               â”œâ”€â”€ gen_00001.png
â”‚               â””â”€â”€ ...
â”‚
â””â”€â”€ scripts/
    â””â”€â”€ run_precompute.py        # One-time ML enrichment (Saturday night)
```

---

## Execution Plan

### Saturday (Foundation + Enrichment Day)

| Time Block | Task | Deliverable |
|------------|------|-------------|
| **Morning (4h)** | Project setup + data download | Working `uv` environment, DuckDB connected, 10K images downloaded |
| **Afternoon (4h)** | Simulation layer | `user_generator.py`, `telemetry_enricher.py` producing realistic fake data |
| **Evening (3h)** | Dagster pipeline + start enrichment | Asset DAG running; kick off `run_precompute.py` to run overnight |

**Saturday Night (runs while you sleep):**
```bash
# Run ML enrichment (~70 min for 10K)
python scripts/run_precompute.py

# This runs:
# 1. CLIP embeddings (~5 min)
# 2. LLM prompt analysis (~50 min)
# 3. K-means clustering (~1 min)
```

**Saturday Checkpoint:** Can run `dagster dev` and see data flowing. Enrichment running in background.

### Sunday (Polish Day)

| Time Block | Task | Deliverable |
|------------|------|-------------|
| **Morning (4h)** | Verify enrichment + build aggregations | Star schema complete with ML features, `fct_sessions` working |
| **Afternoon (3h)** | Dashboard | Streamlit app with 4-5 key visualizations including image viewer |
| **Evening (3h)** | Demo prep | Live query console, talking points, edge case handling |

**Sunday Checkpoint:** Can demo full pipeline + dashboard + run ad-hoc queries + show ML-powered insights.

---

## Demo Script (for interview)

1. **Start the pipeline** (30 sec)
   ```bash
   dagster dev
   # Show the asset graph materializing
   ```

2. **Show raw â†’ enriched â†’ transformed flow** (2 min)
   - Open DuckDB CLI
   - Query raw prompts â†’ show CLIP/LLM enrichment columns â†’ show aggregated sessions

3. **Dashboard walkthrough** (3 min)
   - Friction Score distribution by user tier
   - Session cost trends over time
   - "Power users churn less" visualization

4. **SQL Copilot demo** (2 min) â­ **Wow moment #1**
   ```
   Type: "Show me error rate by user tier"
   â†’ Watch SQL generate in real-time (~0.5s)
   â†’ Review the generated query
   â†’ Click "Run Query"
   â†’ Show results
   ```

5. **Session Explorer** (3 min) â­ **Wow moment #2**
   ```
   Type: "cyberpunk city"
   â†’ See image grid with similar prompts
   â†’ Filter by Status = "timeout"
   â†’ "All timeouts are realistic styleâ€”anime has only 12% errors"
   â†’ Show actual generated images alongside metrics
   ```

6. **Live query â€” connect the insight** (1 min)
   ```sql
   -- Confirm the pattern: realistic cyberpunk = high timeout rate
   SELECT
     art_style,
     COUNT(*) AS total,
     SUM(CASE WHEN status = 'timeout' THEN 1 ELSE 0 END) AS timeouts,
     ROUND(100.0 * SUM(CASE WHEN status = 'timeout' THEN 1 ELSE 0 END) / COUNT(*), 1) AS timeout_pct
   FROM fct_generations g
   JOIN dim_prompts p USING (prompt_id)
   WHERE p.prompt_text ILIKE '%cyber%' OR p.mood = 'dark'
   GROUP BY art_style
   ORDER BY timeout_pct DESC;
   ```

7. **Design decisions** (3 min)
   - Why DuckDB over Postgres (OLAP, zero-config, columnar, native VSS)
   - Why MLX for copilot, sentence-transformers for search (right tool for each job)
   - Why star schema (analytics-friendly, clear separation)
   - Production migration path (DuckDB â†’ MotherDuck/Snowflake, blob â†’ S3)

---

## Tool Preferences

- **Package management**: Use `uv`, not pip
- **Formatting**: Run `ruff format` before committing
- **SQL over Python**: For transformations, write SQL in Dagster assetsâ€”avoid pandas
- **Quick queries**: Use `duckdb` CLI directly, not Python wrappers
- **No notebooks**: All code in `.py` files for demo clarity
- **File creation**: Prefer creating actual files over showing code in chat
- **ML inference**: Use MLX for local, mention vLLM/TGI for production
- **Pre-compute over runtime**: All ML enrichment happens once, stored as columns

---

## Anti-Patterns to Avoid

- âŒ Don't over-engineer the simulation (it's a demo, not a physics engine)
- âŒ Don't add Kubernetes/Docker unless asked
- âŒ Don't build a REST API (this is analytics, not a product)
- âŒ Don't spend time on auth/security (it's a local demo)
- âŒ Don't use pandas for transformations (use SQL/dbtâ€”it shows better instincts)

---

## Open Questions (Decide Early)

1. **Data volume**: Start with 5K (safe) or go for 10K (impressive)?
   â†’ Start with 5K for iteration, scale to 10K Saturday night if enrichment runs smoothly

2. **Dashboard features**: Basic metrics or include image viewer?
   â†’ Include image viewer with similarity searchâ€”it's the "wow" factor

3. **Fallback plan**: What if MLX enrichment fails?
   â†’ Schema still works without ML columns; just skip those demo queries

---

## Resources

- [DiffusionDB on HuggingFace](https://huggingface.co/datasets/poloclub/diffusiondb)
- [DuckDB Python API](https://duckdb.org/docs/api/python/overview)
- [Dagster Tutorial](https://docs.dagster.io/tutorial)
- [Streamlit Docs](https://docs.streamlit.io/)
- [MLX Documentation](https://ml-explore.github.io/mlx/)
- [MLX-LM (Language Models)](https://github.com/ml-explore/mlx-examples/tree/main/llms)
- [MLX Community Models](https://huggingface.co/mlx-community)
- [Qwen2.5 Models](https://huggingface.co/Qwen)

---

## When Stuck

Ask Claude Code for:
1. "Generate the boilerplate for [component]"
2. "Debug this error: [paste traceback]"
3. "Write the SQL for [business question]"
4. "What's the 80/20 way to implement [feature]?"

**Don't ask for:**
- Philosophical debates about architecture
- Comparisons of 5 different tools
- Perfect solutions (we need working solutions)

---

*Last updated: Project Kickoff*
*Target: Luma AI Data Science Interview â€” Screen 1: Project Review*
